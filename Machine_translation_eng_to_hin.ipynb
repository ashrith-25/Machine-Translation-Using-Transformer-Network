{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f49c21",
   "metadata": {
    "id": "a4f49c21"
   },
   "outputs": [],
   "source": [
    "def remove_dublicates(a):\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a)):\n",
    "            if a[i]==a[j] and i!=j:\n",
    "                print((i+1)*j)\n",
    "                a[j] = 0\n",
    "    b = []\n",
    "    for i in range(len(a)):\n",
    "        if a[i]!=0:\n",
    "            b.append(a[i])\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6315b7",
   "metadata": {
    "id": "9d6315b7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79d8bc5",
   "metadata": {
    "id": "d79d8bc5",
    "outputId": "10bcce45-199b-4c53-9964-fadc0d496cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659083\n"
     ]
    }
   ],
   "source": [
    "f = open(\"IITB.en-hi.en\",\"r\",encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "data = []\n",
    "\n",
    "for line in lines:\n",
    "    data.append(line)\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77633744",
   "metadata": {
    "id": "77633744"
   },
   "outputs": [],
   "source": [
    "df_english = pd.DataFrame(data,columns=['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fe0b0c",
   "metadata": {
    "id": "d6fe0b0c",
    "outputId": "c7059847-9558-4f86-ac7c-e2507c7e276e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659083"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014bc881",
   "metadata": {
    "id": "014bc881",
    "outputId": "aa67e78d-9efb-4a23-b9bc-e1b5a92fe6f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give your application an accessibility workout\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accerciser Accessibility Explorer\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The default plugin layout for the bottom panel\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The default plugin layout for the top panel\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A list of plugins that are disabled by default\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentences\n",
       "0  Give your application an accessibility workout\\n\n",
       "1               Accerciser Accessibility Explorer\\n\n",
       "2  The default plugin layout for the bottom panel\\n\n",
       "3     The default plugin layout for the top panel\\n\n",
       "4  A list of plugins that are disabled by default\\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e2555e",
   "metadata": {
    "id": "61e2555e"
   },
   "outputs": [],
   "source": [
    "df_english['sentences'] = df_english['sentences'].apply(lambda x: x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dff4145",
   "metadata": {
    "id": "0dff4145",
    "outputId": "ddef7988-b2a1-47d0-f9d3-507c149288c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659078</th>\n",
       "      <td>The Prime Minister, Shri Narendra Modi has con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659079</th>\n",
       "      <td>In a tweet, the Prime Minister said, congratul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659080</th>\n",
       "      <td>I also congratulate all those who took oath as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659081</th>\n",
       "      <td>The NDA family will work together for the prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659082</th>\n",
       "      <td>I assure all possible support from the Centre ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1659083 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentences\n",
       "0           Give your application an accessibility workout\n",
       "1                        Accerciser Accessibility Explorer\n",
       "2           The default plugin layout for the bottom panel\n",
       "3              The default plugin layout for the top panel\n",
       "4           A list of plugins that are disabled by default\n",
       "...                                                    ...\n",
       "1659078  The Prime Minister, Shri Narendra Modi has con...\n",
       "1659079  In a tweet, the Prime Minister said, congratul...\n",
       "1659080  I also congratulate all those who took oath as...\n",
       "1659081  The NDA family will work together for the prog...\n",
       "1659082  I assure all possible support from the Centre ...\n",
       "\n",
       "[1659083 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2074ec3c",
   "metadata": {
    "id": "2074ec3c",
    "outputId": "81f43098-325a-4638-e2c5-ebd85e22b352"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Highlight duration</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The duration of the highlight box when selecti...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Highlight border color</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The color and opacity of the highlight border.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Highlight fill color</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The color and opacity of the highlight fill.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>API Browser</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences   flag\n",
       "0      Give your application an accessibility workout   True\n",
       "1                   Accerciser Accessibility Explorer   True\n",
       "2      The default plugin layout for the bottom panel   True\n",
       "3         The default plugin layout for the top panel   True\n",
       "4      A list of plugins that are disabled by default   True\n",
       "5                                  Highlight duration  False\n",
       "6   The duration of the highlight box when selecti...   True\n",
       "7                              Highlight border color  False\n",
       "8      The color and opacity of the highlight border.   True\n",
       "9                                Highlight fill color  False\n",
       "10       The color and opacity of the highlight fill.   True\n",
       "11                                        API Browser  False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['flag'] = df_english['sentences'].apply(lambda x: 'True' if type(x)==type(df_english['sentences'][0]) and len(x)>30 and len(x)<90 else 'False')\n",
    "flag = df_english['flag']\n",
    "df_english.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb725aa",
   "metadata": {
    "id": "3bb725aa"
   },
   "outputs": [],
   "source": [
    "f = open(\"IITB.en-hi.hi\",\"r\",encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "data = []\n",
    "string = '' \n",
    "for line in lines:\n",
    "    data.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e71eb0",
   "metadata": {
    "id": "25e71eb0"
   },
   "outputs": [],
   "source": [
    "data = list(zip(data, flag))\n",
    "df_hindi = pd.DataFrame(data,columns=['sentences','flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ba9751",
   "metadata": {
    "id": "c3ba9751",
    "outputId": "37d07d11-f11a-4f40-b8d2-dcdfe29a3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GL पिक्सेल उद्धरण ऑब्जेक्ट के प्रयोग को अक्षम करें\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "error = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q',\n",
    "         'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '(', ')' ]\n",
    "special_char=['/','.','¨','¾','®','¥','à','¤','©','+','=','%','@','#','$','`','~','-','1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "error=error+special_char\n",
    "\n",
    "string = df_hindi['sentences'][45534]\n",
    "string1 ='दायाँ स्थिति इंच अक्षर'\n",
    "string2 = 'GL पिक्सेल उद्धरण ऑब्जेक्ट के प्रयोग को अक्षम करें'\n",
    "def check1(x):\n",
    "    for k in error:\n",
    "        try: \n",
    "            x.index(k.upper())\n",
    "            return False\n",
    "        except ValueError as v:\n",
    "            pass\n",
    "        \n",
    "    return True\n",
    "\n",
    "def check2(x):\n",
    "    for k in error:\n",
    "        try: \n",
    "            x.index(k.lower())\n",
    "            return False\n",
    "        except ValueError as v:\n",
    "            pass\n",
    "        \n",
    "    return True\n",
    "\n",
    "print(string)\n",
    "print(not check1(string) or not check2(string))\n",
    "print(not check1(string1) or not check2(string1))\n",
    "print(not check1(string2) or not check2(string2))\n",
    "df_hindi['flag1'] = df_hindi['sentences'].apply(lambda x: 'False' if (not check1(x)  or not check2(x)) else 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba7409f4",
   "metadata": {
    "id": "ba7409f4"
   },
   "outputs": [],
   "source": [
    "df_hindi['flag2'] = df_hindi['sentences'].apply(lambda x: 'True' if len(x)<90 else 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8529b4ec",
   "metadata": {
    "id": "8529b4ec",
    "outputId": "2f361bb8-7999-4436-abec-cf7211505664"
   },
   "outputs": [],
   "source": [
    "flag1 = df_hindi['flag1']\n",
    "flag2 = df_hindi['flag2']\n",
    "df_hindi = df_hindi[df_hindi['flag'] == 'True']\n",
    "df_hindi = df_hindi[df_hindi['flag1'] == 'True']\n",
    "df_hindi = df_hindi[df_hindi['flag2'] == 'True']\n",
    "df_hindi = pd.DataFrame(df_hindi['sentences'][:300000])\n",
    "df_hindi = df_hindi.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "df_english['flag1'] = flag1\n",
    "df_english['flag2'] = flag2\n",
    "df_english = df_english[df_english['flag'] == 'True']\n",
    "df_english = df_english[df_english['flag1'] == 'True']\n",
    "df_english = df_english[df_english['flag2'] == 'True']\n",
    "df_english = pd.DataFrame(df_english['sentences'][:300000])\n",
    "df_english = df_english.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1094f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         sentences\n",
      "0  अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
      "                                        sentences\n",
      "0  Give your application an accessibility workout\n"
     ]
    }
   ],
   "source": [
    "print(df_hindi.head(1))\n",
    "print(df_english.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dd08125",
   "metadata": {
    "id": "6dd08125",
    "outputId": "99e475ee-2f10-4ce3-ccb1-00d74ee349f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 300000\n"
     ]
    }
   ],
   "source": [
    "print(len(df_hindi), len(df_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe4dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hindi = [f'[start] {sentence} [end]' for sentence in df_hindi['sentences']] \n",
    "English = [sentence for sentence in df_english['sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef90d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "for i in range(len(Hindi)):\n",
    "    text_pairs.append([English[i], Hindi[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19c3083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He devoted much of his energy to the betterment of the people.', '[start] उसने अपनी शक्ति का अधिकांश भाग लोगों के जीवन को बेहतर बनाने में लगाया।  [end]']\n",
      "['We must take destiny in our own hands to build the Indiaof our dreams.', '[start] हमें अपने सपनों के भारत का निर्माण करने के लिए भाग्य को अपनी मुट्ठी में करना होगा। [end]']\n",
      "['Presently four oil refineries have been working in the State including the one at Digboi.', '[start] इस समय राज्य में चार तेलशोधक कारखाने काम कर रहे हैं, जिनमें से एक डिगबोई में है।  [end]']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(3):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41cd340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14562 14793\n"
     ]
    }
   ],
   "source": [
    "max1 = 0\n",
    "max2 = 0\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "for i in range(len(text_pairs)):\n",
    "    if len(text_pairs[i][1])>max2:\n",
    "        max2 = len(Hindi[i])\n",
    "        index2 = i\n",
    "    if len(text_pairs[i][0])>max1:\n",
    "        max1 = len(English[i])\n",
    "        index1 = i\n",
    "        \n",
    "print(index1, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db449e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "length = len(text_pairs)\n",
    "len_val_samples = int(0.01 * length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4c9a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_samples = length - 2*len_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:len_train_samples]\n",
    "val_pairs = text_pairs[len_train_samples:len_train_samples+len_val_samples]\n",
    "test_pairs = text_pairs[len_train_samples+len_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b732e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras as keras\n",
    "import re\n",
    "import string\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace('[', '')\n",
    "strip_chars = strip_chars.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d5464ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 50000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    \n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_hindi_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_hindi_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf495b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94cb14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, hin):\n",
    "    eng = source_vectorization(eng)\n",
    "    hin = target_vectorization(hin)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"hindi\": hin[:, :-1],\n",
    "    }, hin[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, hin_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    hin_texts = list(hin_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, hin_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe9eb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['hindi'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n",
      "tf.Tensor(\n",
      "[[    2    42    35 ...     0     0     0]\n",
      " [    2  4333 10415 ...     0     0     0]\n",
      " [    2  4775     6 ...     0     0     0]\n",
      " ...\n",
      " [    2 31208    11 ...     0     0     0]\n",
      " [    2    13  3008 ...     0     0     0]\n",
      " [    2  9149 11329 ...     0     0     0]], shape=(64, 20), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   42    35   384 ...     0     0     0]\n",
      " [ 4333 10415     4 ...     0     0     0]\n",
      " [ 4775     6   402 ...     0     0     0]\n",
      " ...\n",
      " [31208    11  7499 ...     0     0     0]\n",
      " [   13  3008  2368 ...     0     0     0]\n",
      " [ 9149 11329    10 ...     0     0     0]], shape=(64, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['hindi'].shape: {inputs['hindi'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "    print(inputs[\"hindi\"])\n",
    "    print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f989dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98f8b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c65839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqclass PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shaseqpe(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b522c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"hindi\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69eebcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " hindi (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   12805120    ['english[0][0]']                \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Positi  (None, None, 256)   12805120    ['hindi[0][0]']                  \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   5259520     ['positional_embedding_1[0][0]', \n",
      " erDecoder)                                                       'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 50000)  12850000    ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,875,216\n",
      "Trainable params: 46,875,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee6a1562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3282/3282 [==============================] - 470s 142ms/step - loss: 2.9794 - accuracy: 0.3108\n",
      "Epoch 2/15\n",
      "3282/3282 [==============================] - 487s 148ms/step - loss: 2.3231 - accuracy: 0.4075\n",
      "Epoch 3/15\n",
      "3282/3282 [==============================] - 481s 147ms/step - loss: 2.0085 - accuracy: 0.4633\n",
      "Epoch 4/15\n",
      "3282/3282 [==============================] - 506s 154ms/step - loss: 1.8002 - accuracy: 0.5022\n",
      "Epoch 5/15\n",
      "3282/3282 [==============================] - 513s 156ms/step - loss: 1.6467 - accuracy: 0.5323\n",
      "Epoch 6/15\n",
      "3282/3282 [==============================] - 512s 156ms/step - loss: 1.5235 - accuracy: 0.5573\n",
      "Epoch 7/15\n",
      "3282/3282 [==============================] - 509s 155ms/step - loss: 1.4227 - accuracy: 0.5780\n",
      "Epoch 8/15\n",
      "3282/3282 [==============================] - 508s 155ms/step - loss: 1.3387 - accuracy: 0.5957\n",
      "Epoch 9/15\n",
      "3282/3282 [==============================] - 507s 154ms/step - loss: 1.2657 - accuracy: 0.6124\n",
      "Epoch 10/15\n",
      "3282/3282 [==============================] - 507s 154ms/step - loss: 1.2019 - accuracy: 0.6270\n",
      "Epoch 11/15\n",
      "3282/3282 [==============================] - 508s 155ms/step - loss: 1.1448 - accuracy: 0.6399\n",
      "Epoch 12/15\n",
      "3282/3282 [==============================] - 504s 154ms/step - loss: 1.0934 - accuracy: 0.6520\n",
      "Epoch 13/15\n",
      "3282/3282 [==============================] - 503s 153ms/step - loss: 1.0471 - accuracy: 0.6630\n",
      "Epoch 14/15\n",
      "3282/3282 [==============================] - 508s 155ms/step - loss: 1.0070 - accuracy: 0.6729\n",
      "Epoch 15/15\n",
      "3282/3282 [==============================] - 505s 154ms/step - loss: 0.9696 - accuracy: 0.6820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25642ed4400>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71412336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 58s 82ms/step - loss: 2.1014 - accuracy: 0.5090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1013667583465576, 0.509003758430481]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "139ebc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, embedding_2_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./save\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./save\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(\n",
    "    transformer,\n",
    "    './save',\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None,\n",
    "    save_traces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d9d54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = tf.saved_model.load('./save/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a471f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"sou.csv\", \"w\")\n",
    " \n",
    "str = repr(source_vectorization.get_vocabulary())\n",
    "file.write(str)\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "file = open(\"tar.csv\", \"w\")\n",
    " \n",
    "str = repr(target_vectorization.get_vocabulary())\n",
    "file.write(str)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be0f4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "source_vocab = pd.read_csv('source.csv', header=None)\n",
    "source_vocab = np.array(source_vocab)\n",
    "source_vocab = source_vocab[0]\n",
    "\n",
    "for i in range(len(source_vocab)):\n",
    "    source_vocab[i] = source_vocab[i][2:-1]\n",
    "    \n",
    "\n",
    "target_vocab = pd.read_csv('target.csv', header=None)\n",
    "target_vocab = np.array(target_vocab)\n",
    "target_vocab = target_vocab[0]\n",
    "\n",
    "for i in range(len(source_vocab)):\n",
    "    target_vocab[i] = target_vocab[i][2:-1]\n",
    "target_vocab[-1] = target_vocab[-1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af362841",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "sequence_length = 20\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "target = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "target.set_vocabulary(target_vocab)\n",
    "\n",
    "source = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "source.set_vocabulary(source_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48d694cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "['Manage finances']\n",
      "[start] वित्तीय सेवा प्रबंधन [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "hin_vocab = target.get_vocabulary()\n",
    "hin_index_lookup = dict(zip(range(len(hin_vocab)), hin_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target([decoded_sentence])[:, :-1]\n",
    "        predictions = trans([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = hin_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "# test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "# for _ in range(80,100):\n",
    "#     input_sentence = random.choice(test_eng_texts)\n",
    "#     print(\"-\")\n",
    "#     print(input_sentence)\n",
    "#     print(decode_sequence(input_sentence))\n",
    "    \n",
    "input_sentence = ['Manage finances']\n",
    "print(\"-\")\n",
    "print(input_sentence)\n",
    "print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
